\documentclass[10pt,a4paper]{article}
\usepackage[latin1]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\author{Connor Noyes}
\title{Stochastic Entry Guidance}
\begin{document}
	\maketitle
%	\section{EoM}
%		The 3DOF equations of motion are
%		\begin{align}
%		& \dot{h} = V\sin\gamma \\
%		& \dot{\theta} = \dfrac{V\cos\gamma}{r}\dfrac{\cos\psi}{\cos\phi} \\
%		& \dot{\phi} = \dfrac{V\cos\gamma}{r}\sin\psi \\
%		& \dot{V} = -D-g\sin\gamma \\
%		& \dot{\gamma} = \dfrac{L}{V}\cos\sigma - (g/V-V/r)\cos\gamma \\
%		& \dot{\psi} = \dfrac{L\sin\sigma}{V\cos\gamma}
%		\end{align}
%		
%		The methods employed here must use a fixed final independent value, so either energy, velocity, or trajectory length may be used in place of time. The 3DOF equations of motion with respect to energy are 
%		\begin{align}
%		& h' = -\frac{\sin\gamma}{D} \\
%		& \theta' = -\dfrac{\cos\gamma}{rD}\dfrac{\cos\psi}{\cos\phi} \\
%		& \phi' = -\dfrac{\cos\gamma}{rD}\sin\psi \\
%		& \gamma' = -\dfrac{L}{V^2D}\cos\sigma + (g/V-V/r)\dfrac{\cos\gamma}{VD} \\
%		& \psi' = -\dfrac{L\sin\sigma}{V^2D\cos\gamma} \\
%		& V = f(E,h) = \sqrt{2(E+\dfrac{\mu}{h+r_p})}
%		\end{align}
%		and the longitudinal equations with respect to energy are the reduced set
%		\begin{align}
%		& h' = -\frac{\sin\gamma}{D} \\
%		& s' = -\dfrac{\cos\gamma}{D} \\
%		& \gamma' = -\dfrac{L}{V^2D}u + (g/V-V/r)\dfrac{\cos\gamma}{VD} \\
%		& V = f(E,h) = \sqrt{2(E+\dfrac{\mu}{h+r_p})}
%		\end{align}
		
%	\section{Problem Statement}

%		Outline: Pose the stochastic problem. 
% Show that to first order, the trajectory of mean parameters is the mean trajectory. Second order effects take into account covariance. Thus, we can assume the nominal trajectory determines the mean trajectory, and closed-loop guidance controls the variance. Then, nominal trajectory is continually redesigned via sensitivity method (possibly framed as a mean/covariance argument again), and the loop is closed using feedback linearization (likely using an observer, potentially extended to account for control saturation), showing that minimization of expected errors is a variance minimization problem.

%		The equations of motion governing the vehicle during entry are given by the affine-in-control $\mathrm{It\hat{o}}$ stochastic differential state equation (with respect to energy) and stochastic observation equation
%		\begin{align}
%		&dx = (f(x)+g(x)u)dE + F(x,u)d\omega \label{eq_ito_dynamics}\\
%		&y = h(x) + H(x)d\zeta
%		\end{align}
%		where $\omega,\;\zeta$ are Wiener processes with covariances \textbf{TODO: finish this}. The general entry guidance problem may be posed as finding the control, here taken to be the bank angle profile, that guides an entry vehicle subject to the dynamics in Eq.~(\ref{eq_ito_dynamics}) to some final condition $x(E_f)\in\Omega$. The final condition is very general and has as special cases fully- or partially-specified final states, a target set defined by, e.g., conditions for parachute deployment or supersonic retropropulsion ignition, and optimal control problems where $\Omega$ is defined by an extremal quantity such as maximum crossrange subject to a minimum terminal altitude.
%		
%		The presence of significant uncertainty in the form of initial conditions, system parameters, or process/measurement noise leads to a desire to control not individual trajectories but families of trajectories forming a distribution. In the stochastic setting, Kolmogorov's Forward equation (also referred to as the Fokker-Planck equation)
%		\begin{equation}
%		\dfrac{\partial \mu}{\partial t} = -\mathrm{trace}\left(\dfrac{\partial f}{\partial x} + g(x)\dfrac{\partial u}{\partial x}\right)\mu+ \dfrac{\partial^2 f}{\partial x^2} \label{eq_kolmogorov_forward}
%		\end{equation}
%		governs the evolution of the distribution $\mu(t,x)$. If the Brownian motion term $F(x,u)=0$, the problem becomes deterministic (but still uncertain) and Eq.~(\ref{eq_kolmogorov_forward}) reduces to the Liouville equation
%		\begin{equation}
%		\dfrac{\partial \mu}{\partial t} = -\mathrm{trace}\left(\dfrac{\partial f}{\partial x} + g(x)\dfrac{\partial u}{\partial x}\right)\mu = -\mathrm{div}(f(x)+g(x)u(t,x))\mu. \label{eq_liouville}
%		\end{equation}

	\section{Motivation and Background}
%	Mease advice: give a bird's eye view of the problem. Show the current approach (MSL, linearized range control, separate lateral control, heading alignment) and discuss our approach and how it differs (near-optimal reference trajectories, combined range/lateral control, improved heading alignment, OUU). Can also discuss different UQ options in the loop (STM, QMC, PCE). 
% Pose the full problem, then state how MSL tackled it (and any assumptions), then give our approach (whatever that is...). Then give details, and results.

	Broadly speaking, the goal of entry, descent, and landing is to deliver a spacecraft safely to the planet surface. The modern era of Mars EDL, beginning with the Mars Science Laboratory (MSL) in 2012, utilizes hypersonic entry guidance to reduce the landing footprint and deliver ever-increasing payload mass. The initial state of the vehicle at entry interface will differ from the planned entry state (delivery error), and this state will not be known exactly onboard the vehicle (knowledge error). Additionally, the models used in development of the onboard GNC system (such as the planetary atmosphere and the aerodynamics of the vehicle) are imperfect due to both inaccuracy in the model's parameters as well as the existence of unmodeled (or incorrectly modeled) dynamics.
	
	The Mars Science Laboratory spacecraft demonstrated the first guided entry on Mars when it landed in Gale Crater on August $5^{th}$, 2012. The vehicle flew with an offset center of mass to create an angle of attack that allowed the spacecraft to generate lift. The guidance approach used bank angle commands to orient the resulting lift vector and thereby compensate for dispersions.\cite{MSL_EDL_Overview_JPL}
	
	\subsection{Reference Trajectory}
	MSL used a simple parametrization of the bank angle profile consisting of an initial bank angle, a late bank angle, and a linear change between them ending at a fixed velocity, after which the late bank angle is held until heading alignment begins, see Fig.~\ref{MSLParametrization} for an example. The profile was used to determine the longitudinal motion and a separate lateral logic was used to control crossrange. This decoupling of longitudinal and lateral channels is common throughout entry guidance literature.
	
	
	\begin{figure*}
		\centering
		\includegraphics[width=4in]{MSLProfile.png}
		\caption{\bf{MSL's bank angle parametrization.}}
		\label{MSLParametrization}
	\end{figure*}
	\begin{figure*}
		\centering
		\includegraphics[width=4in]{BankAngleProfile.png}
		\caption{\bf{The proposed bank angle parametrization.}}
		\label{RefTrajParametrization}
	\end{figure*}
	
	\subsection{Entry Guidance}
	MSL's entry guidance consisted of three phases. The first was a pre-bank phase in which the vehicle flew a constant bank angle prior to entering the atmosphere. The phase ended when the drag acceleration exceeded 0.2 Earth g, and the range control phase began. Range control was managed by the Entry Terminal Point Controller (ETPC), a modified version of the Apollo command module entry guidance, to predict the downrange flown and steer the bank angle to rectify range errors. The separate lateral logic referred to earlier consists of a velocity-dependent corridor in which the guidance commands a bank reversal whenever the onboard estimate of crossrange exceeds the corridor threshold. The final phase is heading alignment in which the vehicle is commanded to aim the vehicle toward the target point. This phase began at a fixed velocity which for MSL was 1100 m/s. The magnitude of the bank angle is saturated to 30$^\circ$ to ensure most of the lift generated is countering gravity to maintain adequate chute deployment altitude. Parachute deployment beginning terminal descent was targeted to initiate at a navigated velocity corresponding approximately to Mach 2.
	
	The Apollo guidance adapted for MSL is essentially an LQR approach in which so-called "influence coefficients" are computed and used as gains (scheduled by navigated velocity) for the feedback controller. These gains are computed on the ground via backward integration of the adjoint equations and stored onboard the GNC flight computer.  Rather than perform state feedback directly, the spacecraft states such as altitude and flight path angle are instead replaced by quantities that are better measured or estimated onboard, including aerodynamic drag and altitude rate.
	
	In total, the free parameters in MSL's entry guidance approach consist of the range control gain (sometimes referred to as the overcontrol gain, or $ K_3 $), the bank reversal corridor parametrization, the reference profile generated by the bank angle parametrization, and the velocity at which heading alignment begins. Although not included here, the onboard estimates of drag and lift-to-drag ratio are each filtered with their own time constants \cite{MSL_EDL_Overview_JPL} and these could also be a part of the optimization process.
		%Fisher and Bhattacharya use polynomial chaos expansions and direct optimal control to solve for an open loop control to steer systems with probabilistic parametric uncertainty.
		
%		The ultimate goal in controlling an uncertain system is to steer an entire distribution of states to some final distribution. Although it may be desirable to steer the entire distribution of states. 
		
	% Outline
	% Full stochastic optimal control problem - GNC strategy + reference traj
	% Conversion into stochastic NLP (via control parametrization based on optimal control techniques) then into deterministic NLP (based on QMC, PCE, STM, Unscented)
	\section{Proposed Solution}
	An alternate bank angle parametrization based on optimal control results was developed in Ref.~\cite{High Elevation Landing}. It includes bank reversals in the planning and thus steers both the longitudinal and lateral channels simultaneously. Use of time as the independent variable allows simple imposition of rate and acceleration constraints on the bank angle profile although this was not explored in Ref.~\cite{High Elevation Landing}. The equations of motion are integrated forward using the time-varying bank profile to obtain a drag acceleration reference profile as a function of energy to be used onboard. The version applied here is modified to remove the first switch and bank angle segment, and also introduces rate and acceleration limits to make the commands easier to track. Fig.~\ref{RefTrajParametrization} shows an example profile. Combined planning, although more difficult than in the decoupled scenario, allows for improved performance because bank reversals that are not accounted for will degrade the longitudinal performance. 
	
	The nonlinear predictive controller developed in Ref.~\cite{JoelController} provides a closed-form solution that makes use of the full nonlinear model rather than linearize. We will demonstrate that optimizing the reference profile and controller parameters simultaneously is beneficial compared to separate optimizations or parametric studies.
	
	
	\section{Entry Guidance as an Optimization Under Uncertainty Problem}
	The goal is to design a reference trajectory and feedback controller that together constitute a guidance approach capable of steering the stochastic trajectory evolution to the target in expectation and with minimal variance. Thus we define the problem as finding the feedback control $u\equiv u(x,x_{Ref})$ that minimizes
	\begin{align}
		J = \mathbb{E}[(DR-DR_{Target})^2 + (CR-CR_{Target})^2]
	\end{align}
	subject to 
	\begin{align}
		&\dot{x} = f(x,u) \\
		&x_0 \in \Omega_0 \\
		&x_f \in \mathbb{ X}_f
	\end{align}
	where $\Omega_0$ is the domain of the initial PDF and $\mathbb{ X}_f$ is a terminal set defined according to some criteria. For example, MSL considered a range trigger \cite{RangeTriggerWay} but ultimately settled on a fixed velocity trigger instead such that $\mathbb{ X}_f = \{x: V=V_f \}$ is the set of states $x$ such that the navigated velocity equals the trigger velocity. To see that this cost function reduces both the error in expectation as well as variance, define the horizontal error $e$ such that
		\begin{align}
		e^2(x_f) \equiv (DR-DR_{Target})^2 + (CR-CR_{Target})^2
		\end{align}
	and so $	J = \mathbb{E}[e^2]$. Let $\bar{e} = \mathbb{E}[e]$, and then by rearrangement of the definition of variance of a scalar random variable
		\begin{align}
		\mathbb{V}[e] = \mathbb{E}[e^2] - \bar{e}^2
		\end{align}
	we find that 
		\begin{align}
		J = \bar{e}^2 + \mathbb{V}[e].
		\end{align}
	Thus the cost function is an equal weighting of the square of the mean error and its variance. It is clear that $ J $ is a non-negative function and that a solution achieving the global optimum $ J=0 $ implies every trajectory is successfully driven to the targeted downrange and crossrange. With a parametrization of the bank angle and a chosen controller, the problem reduces to a finite dimensional set of parameters that must be optimized rather than an infinite dimensional optimal control problem.
	
	\subsection{Sketch of the Algorithm}
	At the heart of the approach is the desire to optimize the reference data, controller parameters, and other free parameters simultaneously. This is because an optimal reference profile combined with an optimized controller does not yield optimal performance. In fact, even for simple linear systems, simultaneous optimization of the set-point and feedback can outperform standard design procedures\cite{Boyd}. Given an initial PDF, use the mean initial state for reference computation and perform the following steps:
	\begin{enumerate}
		\item An outer optimization loop first selects the free parameters (e.g., bank angle profile parameters such the switching times $t_i,\;i=1:n$ and bank angles $\sigma_i,\;i=0:n$, as well as any free controller parameters)
		\item Integrate the reference trajectory forward to the final energy
		\item Compute the reference objective function value
		\item If $ J_{ref} < J_{threshold} $: store reference quantities (e.g., drag and its derivatives as functions of energy) and continue to next step;\\
		Else: return to the first step and select new parameters
		\item Integrate (preferably in parallel or vectorized manner) $ N $ trajectories using samples drawn from the initial PDF with closed-loop guidance using the selected parameters from step 1
		\item Optionally, if a metamodeling technique is used, construct a metamodel and evaluate it at $N_{meta}>> N$ points sampled from the initial PDF
%		\item Integrate (preferably in parallel or vectorized manner) $ N_{PCE} $ trajectories using samples drawn from the initial PDF with closed-loop guidance using the selected parameters from step 1
%		\item Construct a polynomial chaos expansion from these $ N_{PCE} $ evaluated trajectories
%		\item Evaluate the PCE model at $ N $ points (typically $N_{PCE}\ll N $) sampled from the initial PDF
		\item Determine the final state $x^i_{f}$ of each trajectory ($i\in \mathbb{N}$, $i<N$ for sampling or $i < N_{meta}$ for metamodel version) by applying the parachute deployment conditions
		\item Evaluate the closed-loop cost estimate by taking the expectation over all trajectories
	\end{enumerate}
		
		

	\subsection{Reference Trajectory Design}
	In order to use a tracking controller to guide the vehicle, a suitable reference trajectory is required. The features of a desirable reference trajectory include high performance in the nominal scenario while leaving appropriate margin for the controller to compensate stochastic uncertainty. Naturally standard optimal control techniques can be used to solve for a trajectory that extremizes some feature(s) such as maximizing final altitude at parachute deployment, or minimizing propellant required to land via retropropulsion technology. It does not offer a clear method to determine how much margin is sufficient for the controller (indeed, the optimization knows nothing about the controller to be used, and different controllers will in general require different amounts of control margin). There is a well-known tradeoff between optimality and robustness. In a system with limited control authority, the optimal solution will use all available control and typically ad hoc methods are used to choose the margins. If more margin than is needed is allocated, performance of the reference trajectory (and of the corresponding mean trajectory of the stochastic system) will deteriorate. Note that the reference trajectory uses $ \mathbb{E}[x_0] $ and can therefore be shown to be a first order approximation to the expected trajectory by Taylor expansion.
	
	The deterministic cost used in generating an optimal reference trajectory is given by 
	\begin{align}
	J_{ref} = -h(V_f) + |CR(V_f)| + \kappa|DR(V_f)-DR^*| \label{eq_cost_deterministic}
	\end{align}
	which seeks to maximize final altitude while reaching approximately zero crossrange at a specified final velocity. Setting $\kappa=1$ seeks a solution that also satisfies a downrange requirement $DR(V_f)=DR^*$ while $\kappa=0$ leads to an optimal solution over all reachable downrange distances. 
	
	 The threshold step prevents evaluation of the closed loop system performance (the most expensive step of the approach) when the reference trajectory is unlikely to produce good results. For example, we may choose a minimum acceptable altitude of 8 km and a maximum crossrange magnitude of 3 km, resulting in $ J_{threshold} = -8 + 3 = -5$ km.
			
	\subsection{Closed Loop Performance Evaluation}		
	In addition to the reference trajectory, the controller parameters must also be chosen judiciously to maximize performance. The cost function used depends on the uncertainty space $ \Delta $ and its support $supp(\Delta)\equiv\Omega$.		
			
	In contrast to the open loop formulation employed by Ref.~\cite{RSOptimalControl} where a single control trajectory aims to steer an entire tube of trajectories, we consider the closed loop formulation where both the reference (which in some sense constitutes the open loop portion of the control) and the controller parameters (for an a priori chosen controller design) are optimized. (Part of the optimization could even be selecting which controller is best!)		
	
	Three approaches to the forward uncertainty quantification have been investigated: linear sensitivity (aka state-transition method), (quasi-)Monte Carlo sampling methods, and polynomial chaos expansions.  
			
	\subsection{Entry Termination Logic}
	The logic which decides when the entry phase ends is instrumental to good horizontal performance while ensuring the vehicle is in an appropriate state to deploy a parachute or otherwise initiate a final descent phase. Compare the horizontal accuracy results of a fixed energy trigger (Fig.~\ref{EnergyTrigger}), in which all trajectories end at the same energy as the reference trajectory, and a range trigger with minimum altitude and minimum/maximum velocity limits applied (Fig.~\ref{RangeTrigger}). 
	
		\begin{figure*}
			\centering
			\includegraphics[width=4in]{EnergyTrigger.png}
			\caption{\bf{Downrange vs crossrange results from a Monte carlo with a fixed energy trigger.}}
			\label{EnergyTrigger}
		\end{figure*}
		\begin{figure*}
			\centering
			\includegraphics[width=4in]{SmartTrigger.png}
			\caption{\bf{The same results as Fig.~\ref{EnergyTrigger} with a range trigger used instead. }}
			\label{RangeTrigger}
		\end{figure*}
	
	Unfortunately, the if-then logic used to check altitude and velocity limits in the range trigger is not differentiable, and thus precludes the use of linearization or polynomial chaos to expand the cost function directly. Instead, the state trajectories themselves are expanded. This allows the non-smooth trigger to be incorporated while still leveraging the surrogate models. For polynomial chaos, the downside is that considerably more models must be constructed (one for each state variable used in the cost function, times a fraction of the total number of timesteps taken during integration). The models are then sampled extensively (because the cost to do so is low relative to the cost of constructing the models) and the trigger logic is applied to each trajectory to determine its final state. Then, the expectation of the cost is simply estimated as $\mathbb{E}[J]\approxeq\frac{1}{N}\sum_{i=1}^{N}J(x_f^i)$. 
			
			
	\subsection{Global Optimization}
	The resulting optimization problem may be solved by a number of methods and is not the intended focus here. Both differential evolution and particle swarm optimization have been used to successfully solve this problem. Convergence in both cases can be improved by seeding the initial population with appropriate guesses. 	
	
	\section{Uncertain Optimal Control via Continuation}
	
	Many deterministic optimal control problems can be solved via existing methods, both direct and indirect. When the dynamics are uncertain, due to variations in system parameters or initial state, the problem becomes more challenging. Ref.~\cite{RSOptimalControl} presents an extension of the pseudospectral methodology for numerical optimal control to the uncertain case. Ref.~\cite{UncertainOptimalControl} also used a direct method to solve sample average approximations to the stochastic problem. Ref~\cite{PCE_OCP_Bhattacharya} used Galerkin projection to expand the uncertain system via polynomial chaos expansions, and the resulting deterministic problem of increased dimensionality is solved by a direct method based on B-spline approximations. In contrast to these direct methods, we propose an indirect method based on an extension of Pontryagin's maximum principle (note that Ref.~\cite{UncertainOptimalControl} did develop necessary conditions for optimality despite solving the problem directly), continuation, and sample average approximations. 
	
	Consider systems with dynamics given by deterministic ordinary differential equations. Due to uncertainty in the system's initial state and parameters of the model, evolution of the trajectories is stochastic. Let the joint PDF of the uncertainty space be $ \Delta $ and its support $\mathrm{supp}(\Delta)\equiv\Omega$, and denote by $\mathbb{E}[\cdot]$ the expectation taken with respect to $\Delta$. Both $\Delta$ and $\Omega$ vary with time. Let $\tau=t/t_f$, and define $(\cdot)'=\dfrac{\partial}{\partial\tau}$. By appending the uncertain model parameters to the state vector, all uncertainty can be lumped into the initial conditions without loss of generality and thus each realization of $x_0\in\Omega(\tau=0)=\Omega_0$ admits a deterministic solution $ x(\tau;x_0)$ (in general the dependence on $ x_0 $ is suppressed). Thus the state $x: [0,1]\times\Omega\mapsto\mathbb{R}^n$ and control $u: [0,1]\mapsto U\subseteq\mathbb{R}^m$. The uncertain optimal control problem we seek to solve minimizes the expectation of a functional in Mayer form subject to deterministic dynamics $f: \mathbb{R}^n \times U \mapsto \mathbb{R}^n$ with stochastic initial conditions, control constraints, and a free final time $t_f$:
	\begin{align}
	\min_{u\in U} \;&J = \mathbb{E}[\varphi(x(1))] \label{eq_generic_uncertain_mayer}  \\
		&x' = f(x,u)\cdot t_f \\
		&x(0)= x_0 \in \Omega_0 %\\
		%&u(t,x) \in U
	\end{align}	
	Consider an N-sample average approximation to Eq.~\ref{eq_generic_uncertain_mayer} given by		
	\begin{align}
	J_N = \frac{1}{N}\sum_{i=1}^{N}\varphi(x_i(1)). \label{eq_saa_mayer}
	\end{align}	
	Ref.~\cite{UncertainOptimalControl} showed that under appropriate conditions Eq.~\ref{eq_saa_mayer} produces a meaningful approximation to Eq.~\ref{eq_generic_uncertain_mayer}. 
	
	For each $x_0 \in \Omega_0$, there is an associated costate $\lambda(\tau, x_0)$ such that the Hamiltonian is given by $H(\tau, x_0) = f\cdot\lambda$, the costates satisfy
	\begin{align}
	\dot{\lambda} = -\nabla_xH
	\end{align} 
	and the optimal control is given by (Ref.~\cite{RSOptimalControl})
	\begin{align}
	u^*(t) = \arg\min_{u\in U}\mathbb{E}[H] \label{eq_uncertain_optimal_control}
	\end{align}
	
	An N-sample average approximation to the expected Hamiltonian may also be made
	\begin{align}
		H_N(\tau) = \frac{1}{N}\sum_{i=1}^{N}H(\tau,x_i(0)). \label{eq_saa_hamil}
	\end{align}	
	\textbf{Remark.} \textit{The uncertain scenario is significantly more difficult to solve via indirect method because we seek a single control function to steer the entire tube of trajectories, i.e. the optimal control minimizes the expected Hamiltonian at each point in time. In the deterministic case, the control can be computed from the costates given a guess at the initial costate values. In the uncertain case, using N-sample averages necessitates an initial guess of N sets of initial costate values . Fortunately, via a homotopy method we can exploit nearness of solutions.}	
		
		
	In the case that $\Delta$ is a Dirac distribution supported at some nominal value $x_0$ then the uncertain problem reduces to a standard optimal control problem. This inspires the idea to define a homotopy between the standard OCP and the uncertain OCP. The only requirement is that the domain of the initial PDF is star convex. (A subset $ \mathbb{X} $ of $\mathbb{R}^n$ is star convex if there exists an $ x_0 \in\mathbb{ X}$ such that the line segment from $x_0$ to any point in $ \mathbb{X} $ is contained in $ \mathbb{X} $.) This is a rather weak condition, satisfied by any joint distribution comprising common distributions such as normal, uniform, beta, etc.
	
	\textbf{To be continued - the remaining necessary conditions must be extended, and an approach to solution of this problem must be developed.}

	\section{Covariance Control of Nonlinear Systems via Successive Convexification}
	\textbf{This paper presents a numerical approach for solving a class of nonlinear optimal control problems in which the cost functional is a convex function of the system's covariance. Using recent advances in successive convexification, the original nonlinear optimal control is solved via a convergent sequence of convex subproblems. The state transition matrix used to propagate the system's covariance is the solution to a linear, time-varying differential equation and thus fits naturally into a convex approach. A Chebyshev pseudospectral method is used to transcribe each optimal control subproblem into a convex optimization problem and displays benefit relative to typical discretization methods (i.e. the integration methods of Euler and Heun). Numerical results are presented to demonstrate the approach.}
	
	\subsection{Introduction}
	% One point to consider: we can swap the independent variable to normalized time, or energy etc and consider a fixed final IV problem. The change of variable increases the nonlinearity but we are iterating over linearizations anyway so it is possible.


	% Review of covariance minimization or control 
		% Bhattacharya paper with PCE 
		
	% Review of successive convex optimization (Mao's papers, lossless paper(s)?)
	
	% Review of pseudospectral methods (and the one paper that combines it with convex optimization)
		% Include mention of hp-mesh adaptive NLP and indicate that the results apply here as well 
		
	\subsection{Problem Statement}
	% We can also consider covariance based constraints with a different convex objective function
	
	The systems under consideration are subject to uncertain continuous nonlinear dynamics 
	\begin{align}
	\dot{x}(t) = f(x(t),u(t),\lambda)
	\end{align}
	where $x:[0,t_f]\mapsto \mathbb{R}^n$ is the state trajectory, $u:[0,t_f]\mapsto \mathbb{R}^m$ is the control input, and $\lambda\in\mathbb{R}^p$ is a constant vector representing the parametric uncertainty. It is assumed the control is subject to a convex constraint $u(t) \in U \subset \mathbb{R}^m$, and that the parameters are jointly distributed according to a multivariate normal with zero mean and covariance $P_{\lambda}$, i.e. $\lambda \sim \mathcal{N}(0,P_{\lambda})$. The initial condition uncertainty is similarly described by $x_0 \sim \mathcal{N}(\bar{x}_0,P_{x_0})$ and the total initial covariance is then $P(0) = \left[\begin{array}{cc}
		P_{x_0}& 0_{n\mathrm{x}p} \\
		 0_{p\mathrm{x}n} & P_{\lambda}
		\end{array}\right]$.
	
	An extended state transition matrix (STM) is used to propagate the covariance along a trajectory. Denote by $\Phi^X(t,0) \in \mathbb{R}^{n\mathrm{x}n}$ the standard STM, i.e. the matrix of sensitivities of the state $ x(t) $ to changes in the state $x(0)$. Similarly, $\Phi^P(t,0) \in \mathbb{R}^{n\mathrm{x}p}$ is the matrix of sensitivities of the state $ x(t) $ to the uncertain parameters. Then the extended STM $\Phi(t,0)\in\mathbb{R}^{(n+p)\mathrm{x}(n+p)}$ which includes parametric uncertainty and its governing dynamics may be written succinctly as 
	\begin{align}
	&\Phi \triangleq \left[\begin{array}{cc}
	\Phi^X & \Phi^P \\
	 0_{p\mathrm{x}n} & I_{p\mathrm{x}p}
	\end{array}\right] \\
	&D \triangleq \left[\begin{array}{cc}
		f_x & f_p \\
		 0_{p\mathrm{x}n} & 0_{p\mathrm{x}p}
		\end{array}\right] \\
		&\dot{\Phi}(t,0) = D\Phi(t) \label{eq_stm_dyn} \\
		&\Phi(0,0) = I
	\end{align}
	where the partial derivative matrix $ D $ is evaluated along a trajectory $(x(t),u(t),\lambda)$. Notice that Eq.~(\ref{eq_stm_dyn}) is an LTV system. Let $(x^*(t),u^*(t),\lambda^*)$ be the trajectory along which Eq.~(\ref{eq_stm_dyn}) is integrated with $x^*(0)$ and $\lambda^*$ assuming their expected values ($x^*(0)=\bar{x}_0$ and $\lambda^*=0$) and define the deviations from this trajectory as
	\begin{align}
	\delta x(t) &= x(t)-x^*(t) \\
	%\delta u(t) = u(t)-u^*(t) \\
	\delta \lambda &= \lambda-\lambda^*
	\end{align}
	and finally let $z = [x(t),\,\lambda]$. Consider the first few terms of a Taylor series expansion of $\delta x(t) $ about $ (x^*(t),u^*(t),\lambda^*) $:
	
	\begin{align}
	\delta x(t) &= \dfrac{\partial x(t)}{\partial x(0)} \delta x(0) + \dfrac{\partial x(t)}{\partial \lambda} \delta \lambda \nonumber\\
	&+\frac{1}{2}\left(\delta x^T(0) \dfrac{\partial^2 x(t)}{\partial x^2(0)} \delta x(0) + 2\delta x^T(0)\dfrac{\partial^2 x(t)}{\partial x(0)\partial\lambda} \delta \lambda +\delta \lambda^T \dfrac{\partial^2 x(t)}{\partial \lambda^2} \delta \lambda\right) \label{eq_taylor_exp}\\
	\nonumber &+ h.o.t.
	\end{align}
	Truncating the series at first order provides a linear approximation to Eq.~(\ref{eq_taylor_exp}) that can be written simply as:
	\begin{align}
	\delta z(t) \approx \Phi(t,0)\delta z(0).
	\end{align}
	Let $\mathbb{E}[\cdot]$ be the expectation operator and noting that $\mathbb{E}[z]=0$ we arrive at the linearized estimate of the covariance
	\begin{align}
	P(t) &= \mathbb{E}[\delta z(t)\delta z^T(t)] &\mathrm{ (definition)} \\
	     &= \mathbb{E}[\Phi(t,0)\delta z(0)\delta z(0)^T\Phi^T(t,0)] &\mathrm{(substitution)} \\
	     &= \Phi(t,0)\mathbb{E}[\delta z(0)\delta z(0)^T] \Phi^T(t,0) \\
	     &= \Phi(t,0)P(0)\Phi^T(t,0) &\mathrm{ (definition)}
	\end{align}
	where the third equality is due to the linearity of $\mathbb{E}$ and because $\Phi$ is a deterministic quantity. Hence we define the following optimal control problem:
	\\\\
	\textbf{Problem 1 - Linear Covariance}. \textit{Determine a control function $ u^* $ and state trajectory $ x^* $ that minimize the functional}
	\begin{align}
	J(x,u) = M(P_x(t_f)) + \int_{0}^{t_f}L(P_x(t))\mathrm{d}t
	\end{align}
	\textit{	subject to the constraints }
	\begin{align}
	&\dot{x}(t) = f(x(t),u(t),\lambda) \\
	&x(0) = \bar{x}_0 \\
	&\dot{\Phi}(t) = D\Phi(t) \\
	&\Phi(0) = I \\
	&u(t) \in U
	\end{align}
	\textit{where} $ M:\mathbb{R}^{n\mathrm{x}n}\mapsto \mathbb{R} $ \textit{is the Mayer (terminal) cost,}  $L:\mathbb{R}^{n\mathrm{x}n}\mapsto\mathbb{R}$ \textit{ is the Lagrange (running) cost and both are convex.}
	
	\textit{Remark}: The covariance may be propagated in other ways. The STM differential equations in Problem 1 represent $n(n+p)$ equality constraints. For $n>2$, the unscented transform offers a more efficient (and often more accurate) estimate of the covariance and also captures variance effects of $\lambda$ on the mean using $2(n+p)$ additional constraints.
	\\\\
	\textbf{Problem 2 - Unscented Covariance}. \textit{Determine a control function $ u^* $ and state trajectory $ x^* $ that minimize the functional}
		\begin{align}
		J(x,u) = M(P_x(t_f)) + \int_{0}^{t_f}L(P_x(t))\mathrm{d}t
		\end{align}
		\textit{	subject to the constraints }
		\begin{align}
		&\dot{x}^i(t) = f(x^i(t),u(t),\lambda^i) \\
		&x^i(0) = x^i_0 \\
		&u(t) \in U
		\end{align}
		
	Unlike Problem 1, which assumes that $\mathbb{E}[x(t)] = \mathbb{E}[x(0)] + \int_{0}^{t} f(x,u,\mathbb{E}[\lambda]) d\tau = \bar{x}_0 + \int_{0}^{t} f(x,u,0) d\tau$, Problem 2 constructs a higher order estimate of the mean trajectory.
	
	% Essentially, in the linear approach, final constraints are on the nominal (lambda=0) trajectory (which is a first order approximation to the mean), while in the UKF the constraints are enforced on the second order approximation to the mean.
	
	% discussion of possible cost functions here 
	Trace alone is bad here.
	
	\subsection{Solution Methodology}
	Under the state assumptions, only the nonlinear dynamics render Problem 1 nonconvex.
	
	
	

	
	\subsubsection{Chebyshev Pseudospectral Transcription}
	The idea behind spectral methods is to approximate solutions (here, state-control trajectories $ (x(t),u(t) $) by a finite sum $x(t) \approx x_N(t) = \sum_{i=0}^{N}b_i\phi_i(t)$ where $b_i$ are the coefficients and $ {\phi_k} $ is a chosen set of basis functions. A wide range of polynomial bases have been researched. 
	
	In general, the use of a pseudospectral (PS) method converts the infinite-dimensional optimal control problem into a finite nonlinear programming problem. Due to the underlying convexity of the subproblems posed here, however, the pseudospectral method will result in a convex program and thus can be solved efficiently and to global optimality by a primal-dual interior point method. The idea to utilize a PS method with convex optimization appears to originate in Ref.~\cite{PS_Convex} where Flipped Radau and Lobatto methods are introduced and used to solve a planetary landing problem. Ref.~\cite{PS_Convex_ascent} used a Gauss PS method to solve an optimal ascent trajectory problem. 
	
	Neither of these applications to convex optimization referred to another strength of pseudospectral methods: hp-adaptive mesh based solutions. This well developed theory can be leveraged here as in original NLP context. \textbf{Cite Anil papers here}
	
	% RefPS Convex showed both superior solutions (as measured by the objective function) for an equal number of nodes, as well as greater consistency with the integrated solution ( as measured by the mean and maximal errors between the two solutions)
	
	% Spectral convergence for smooth problems
	% Mesh adaptation for non-smooth problems 
	
	\subsection{Numerical Results}

	

	
	\begin{thebibliography}{1}
%		\bibitem{brockett2012}
%		R. W. Brockett. ``Notes on the control of the Liouville equation'' In P. Cannarsa and J. M.
%		Coron, editors, Control of Partial Differential Equations, pages 101-129. Springer,
%			Berlin-Heidelberg, 2012.
		

		\bibitem{MSL_EDL_Overview_JPL}
		A.D. Steltzner, P.D. Burkhart, et. al, ``Mars Science Laboratory Entry, Descent, and Landing System Overview," 2010.
				
		\bibitem{MSL_EG_Mendeck_Craig}
		G.F. Mendeck, and L.E. Craig, ``Entry Guidance for the 2011 Mars Science Laboratory Mission," AIAA, 2010.
		
		\bibitem{High Elevation Landing}
		G. Duan, A. Bombelli, J. Benito, and K.D. Mease, ``High Elevation Planner for Mars Entry," AIAA, 2016.
		
		\bibitem{RangeTriggerWay}
		D. Way, ``On the use of a Range Trigger for the Mars Science Laboratory Entry Descent and Landing"
		
		\bibitem{JoelController}
		J. Benito, K.D. Mease, ``Nonlinear Predictive Controller for Drag Tracking in Entry Guidance," AIAA/AAS Astrodynamics Specialist Conference and Exhibit, 2008.
		
		\bibitem{RSOptimalControl}
		I.M. Ross, R.J. Proulx, M. Karpenko, and Q. Gong, ``Riemann-Stieltjes Optimal Control Problems for Uncertain Dynamic Systems," AIAA Journal of Guidance Control and Dynamics, 2015.
		
		\bibitem{UncertainOptimalControl}
		C. Phelps, J.O. Royset, and Q. Gong, ``Optimal Control of Uncertain Systems Using Sample Average Approximations," SIAM Journal on Control and Optimization, 2016.
		
		\bibitem{PCE_OCP_Bhattacharya}
		J. Fisher, R. Bhattacharya, ``Optimal Trajectory Generation with Probabilistic System Uncertainty Using Polynomial Chaos," ASME Journal of Dynamic Systems Measurement and Control, 2011.
		
		\bibitem{Boyd}
		S. Boyd, C. Crusius, A. Hansson, ``Control Applications of Nonlinear Convex Programming"
		
		\bibitem{UT}
		S. Julier, J. Uhlmann, ``A General Method for Approximating Nonlinear Transformations of Probability Distributions" 1996.
		
		\bibitem{PS_Convex}
		M. Sagliano, ``Pseudospectral Convex Optimization for Powered Descent and Landing," Journal of Guidance, Control, and Dynamics, 2017.
		
		\bibitem{PS_Convex_ascent}
		Cheng, X., Li, H., and Zhang, R. ``Efficient ascent trajectory optimization using convex models based on the newton-kantorovich/pseudospectral approach". Aerospace Science and Technology, 2017.

		
	\end{thebibliography}
\end{document}