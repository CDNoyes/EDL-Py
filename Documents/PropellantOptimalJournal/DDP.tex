%\documentclass[letterpaper, paper,11pt]{AAS}
\documentclass[journal ]{new-aiaa}
\usepackage[utf8]{inputenc}
\usepackage{textcomp}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage[version=4]{mhchem}
\usepackage{siunitx}
\usepackage{longtable,tabularx}
\setlength\LTleft{0pt} 
%\newtheorem{theorem}{Theorem}
%\newcommand{\deg}{\ensuremath{^{\circ}}}
\newcommand{\state}{\ensuremath{\mathbf{x}}}
\newcommand{\control}{\ensuremath{\mathbf{u}}}
\newcommand{\ur}{\ensuremath{u_{\mathrm{ref}}}}
\newcommand{\State}{\ensuremath{\mathbf{X}}}
\newcommand{\Control}{\ensuremath{\mathbf{U}}}
%\newcommand{\costate}{\mathbf{p}}
%\newcommand{\multiplier}{\mathbf{\lambda}}
\newcommand{\param}{\ensuremath{\mathbf{p}}}
%\newcommand{\costate}{\mathbf{\lambda}}
%\newcommand{\multiplier}{\mathbf{\nu}}
\newcommand{\E}[1]{\mathbb{E}\left[#1\right]}
\newcommand{\V}[1]{\mathbb{V}[#1]}
\newcommand{\mean}{\mathbf{m}}
\newcommand{\cov}{C}
\newcommand{\std}{S}
\newcommand{\sample}{\ensuremath{\mathbf{z}}}
% Title Page
\title{Mars Entry Guidance for High Elevation Landing via Robust Optimal Control}


\begin{document}
\author{Connor D. Noyes\thanks{Ph.D. Candidate, Department of Mechanical and Aerospace Engineering, University of California, Irvine, 92697} \ and Kenneth D. Mease\thanks{Professor Emeritus, Department of Mechanical and Aerospace Engineering, University of California, Irvine, 92697}}
\maketitle

% Refences with notes:
% AltitudeUnderUncertainty
% MarsEntryDesensitized % linearized but closed-loop with fixed gain. No consideration of saturation 
% EntryOUU % Does not use linearization, also doesnt solve in a conventional way, maybe remove?
% TrajectoryDesensitization % Desensitized like seywald and kumar, is it entry applied?
% EntryOUUThesis1 % Earth EDL, focuses on footprint computation, only considers open-loop in the reentry problem, and did not conduct monte carlo to confirm
% EntryOUUThesis2 % LQR, minimum control effort objective which stays away from the bounds, angle of attack as control variable. Does perform Monte Carlo(s) to validate improvement. 

%Rather than impose a chance constraint to reduce saturation, the control is freely allowed to saturate as long as the end result is good. 

% Ideas to discuss:
% Result of optimization is a reference trajectory and (linear) controller with desirable robustness 
% Demonstrate correlation between UT and MC results to validate use of the UT, but there are limits 
% Joint optimization (naturally) leads to superior results over fixed gains 
% Examine how solutions change with weights, and also initial state vs parametric uncertainty 
% Conclusions? New approach to combined reference + controller design 

% Mease notes to implement:
% discuss why velocity is the IV (removes constraint + better DDP convergence)
% Better discussion of how MSL/M2020 do their guidance design 

%TODO: Discuss nominal, mean, robust, etc. Terminology
\section*{Abstract}
%The current generation of Mars entry vehicles employ a modified Apollo entry guidance that performs range control until a fixed velocity, and key trajectory metrics are the total range error and altitude at parachute deploy. We pose the entry guidance problem as an optimal control problem under uncertainty with closed-loop dynamics to determine a reference trajectory that is optimal with respect to an objective that weights mean altitude, altitude variance, and distance variance. The result is a reference trajectory with optimal margin for a given set of input dispersions. The robustness and optimality are validated in a Monte Carlo analysis. Novel aspects of the approach include use of the unscented transform to account for saturation in the closed-loop system, and the use of differential dynamic programming to solve the challenging nonlinear optimal control problem. 
\section*{Introduction}
%The first paragraph is about the future mission requirements driving entry guidance. 

%The second paragraph reviews the state-of-the-art and identifies potential limitations for meeting the new requirements. 

%Then the next paragraph will present the purpose of the paper, namely to present the proposed method, how it may lead to improvements, and how it will be tested. 
\lettrine{N}{ASA} has successfully landed five rovers on Mars to date, including the Mars 2020 rover Perseverance. Each of these missions chose low elevation targets between -1 and -4 km relative to the Mars areoid surface, commonly referred to as MOLA, after the instrument that was used to map Mars' topography. The thin Martian atmosphere makes aerodynamic deceleration ineffective except at lower altitudes where the atmospheric density is thickest. Coupled with the increasing entry mass required to deliver larger, more capable rovers to the surface, this makes high elevation landing on Mars a challenging problem. Nevertheless, targets above 0 km MOLA, like Terra Sirenum in the Southern Highlands, are motivated by scientific interest \cite{MarsWater}. In order for descent and landing operations to have sufficient timeline margin \cite{BraunMarsEDL,MSL_EDL2} when targeting such elevations, the entry phase must be terminated at a sufficiently high altitude.
%The entry phase may be terminated at a fixed velocity \cite{MSL_EDL2}, at a fixed downrange distance \cite{TriggerComparison2020}, or by a more complex function of the vehicle state \cite{LuAdaptiveEDL}. 
%Increasing altitude at a terminal entry state allows the vehicle to target landing sites at higher elevations, which are motivated by reasons of scientific interest, by increasing the time available for subsequent descent and landing operations, which is particularly important in high ballistic coefficient vehicles. 
In addition to terminal entry altitude, guidance designers must also consider additional objectives, such as reducing the size of the landing footprint. Often these multiple objectives are competing, as for example was the case on the Mars Science Laboratory (MSL) mission, where the entry guidance designers noted that targeting landing site elevations above -1 km MOLA would incur a larger landing ellipse \cite{MSL_EDL2}. 
Future missions, including sample return \cite{MSR} and manned, will place even greater emphasis on robust performance than the current generation, such as pinpoint landing requirements \cite{EvolvableMars}. 

The state-of-the-practice for Mars entry guidance is represented by Mars 2020, which inherited its entry guidance architecture from MSL \cite{M2020_EDL}. One change from MSL is that Mars 2020 initiated parachute deployment based on a range trigger rather than a velocity trigger \cite{TriggerComparison2020}. 
MSL and Mars 2020 both utilized the entry terminal point controller (ETPC) \cite{MSL_EDL, M2020_EDL}, a modified version of the Apollo second phase guidance \cite{MSL_EDL2}.
The three phases of the ETPC are prebank, range control, and heading alignment. In the prebank phase, there is no guidance, but the vehicle is rotated to the bank angle predicted at the start of the range control phase.
Range control begins when the vehicle sensed drag acceleration exceeds a threshold value, and steers the vehicle to the correct downrange distance. Lateral guidance operates during range control to command bank reversals when a measure of the crossrange to the target exceeds the deadband threshold which is a quadratic function of velocity. At a specified velocity, guidance switches from range control to heading alignment, during which the bank angle is commanded to reduce the crossrange error.

%TODO: Discuss possibility to handoff to SRP or something other than HeadingAlign/Parameter

The guidance design method used on these latest Mars entry vehicles \cite{MSL_EDL2,M2020_EDL} can be divided into two steps. The first step is to design a reference bank profile and compute the corresponding reference trajectory via open-loop simulation. In the second step, closed-loop performance using the reference trajectory is evaluated under off-nominal conditions. A number of stress trajectories are simulated in closed-loop using the reference trajectory, ETPC gains, and heading alignment guidance to estimate off-nominal performance prior to Monte Carlo simulation \cite{MSL_EDL2}. The results are judged based on the vehicle state at parachute deployment. The free parameters of the guidance algorithm, including the overcontrol gain and heading alignment gain, are then tuned until the performance is satisfactory. If the necessary performance cannot be achieved through tuning, the reference trajectory is changed and the process is repeated. 
%Because the range control gains are determined by the reference trajectory in the ETPC guidance algorithm, proper design of the reference is the most important part of the guidance design. 
The reference bank angle profile is parametrized using an early bank angle, a late bank angle, and the velocities at which the linear (in velocity)) ramp from early to late bank angle begin and end. This variable bank angle profile was a modification from the original Apollo algorithm, which assumed a constant bank angle reference profile. This change allows for more flexibility in trajectory design including enabling higher parachute deploy altitudes \cite{MSL_EDL2}.
%The free parameter affecting range performance is the range overcontrol gain, a multiplier on the predicted range error. 

% Overview and proposed benefits:
In this work we present an alternative to the range controller used by MSL and Mars 2020. Like MSL and Mars 2020, we consider the guidance approach as bank angle modulation using reference trajectory-based feedback control. Briefly, our approach differs by posing the design of the reference trajectory as the solution to a robust optimal control problem, in which the objective is a weighted combination maximizing altitude and minimizing altitude dispersions and range dispersions. Novel aspects of our approach relative to MSL and Mars 2020, as well as other previous work, are described further in the following paragraphs. The design process is substantially simplified by accounting for uncertainty in the generation of the reference trajectory; in particular, Monte Carlo simulations with numerous candidate reference trajectories (\cite{MSL_EDL2}) are avoided in the design process. Monte Carlo simulation is only used post-design to validate and characterize the performance of the resulting guidance. No change is considered for the prebank phase or lateral controller.

Like MSL and Mars 2020, we design the reference trajectory and bank control down to the nominal parachute deployment velocity of 460 m/s, and judge the guidance performance of the reference trajectory and control based on the delivery state at 460 m/s. As mentioned earlier, MSL entry guidance switches from range control (Phase 2) to heading alignment guidance (Phase 3) at v = 1100 m/s. For our assessment, we only consider the Phase 2 entry guidance with $v_f = 460$ m/s, and, for now, leave open the decision on whether or not there should be a switch to a Phase 3 guidance. We note that one of the reasons given for switching to heading alignment for MSL/Mars2020 is that, the range controller does not continue to reduce the range error effectively at velocities below 1100 m/s; the results, presented later in this paper, show that the robust optimal guidance does continue to reduce the range error.

% rationale for not having heading alignment phase: we are addressing high chute deploy altitude with small 3sig deviation directly throughout entry, so there is no need to restrict the bank below 1100 m/s for this reason. Also the results show that range error can be further reduced below 1100 m/s, so we don’t want to give up on this objective. 

% Attributes in more detail and relative to MSL/M2020
%As with MSL, the entry guidance goal is to balance the lift of the vehicle to reduce range error while achieving a safe parachute deploy altitude \cite{MSL_EDL2}. 
We consider a broad class of bank angle profiles by optimizing the objective over piece-wise constant bank profiles with many segments. In the same way that MSL expanded the bank angle profiles under consideration to meet mission requirements, optimizing over a broader class of profiles is a natural extension to meet the stringent requirements of future missions. 

Uncertainty is quantified using the unscented transformation (UT) \cite{UT1997}, allowing both means and standard deviations of key variables to be considered in the objective function of the optimal control problem, thereby formally addressing robust performance. The UT is a deterministic sampling-based method that propagates a set of sample points through the nonlinear dynamics and reconstructs the mean and covariance from the transformed samples. These sample trajectories allow us to estimate closed-loop guidance performance without Monte Carlo simulation, and thus serve a purpose similar to the MSL stress cases. The UT facilitates a computationally feasible approach to reference bank profile design accounting for uncertainty in a single step, rather than the MSL two-step process.

We demonstrate that, with a reformulation of the objective function, the optimal control problem can be solved using differential dynamic programming (DDP) \cite{DDP}, a shooting method originally devised to solve unconstrained nonlinear optimal control problems that has since seen numerous extensions, including to constrained problems \cite{DDP_ControlLimited,HDDP1,HDDP2,DDP_NonlinearConstraints,DDP_InteriorPoint}, and stochastic problems \cite{iLQG, DDP_Stochastic, ozaki_UT,ozaki2020tube}. 



% TODO: Talk about related work 
%Due to the interest in high elevation landings, guidance algorithms designed to maximize altitude at parachute deploy have been studied using optimal control theory 
%\cite{AltitudeOptimization,AltitudeOptimizationIndirect}. Reference \cite{GuangfeiDissertation} proposed an optimization-based onboard trajectory planning method based on a low-order parametrization designed to achieve high altitude at parachute deploy.
%Typically these papers consider predictive guidance and use parametrizations of the bank angle designed to yield high altitudes. 

Approaches to entry guidance based on optimal control formulations that weight a nominal (or mean) optimization objective with covariance terms have been studied in \cite{AltitudeUnderUncertainty, EntryOUUThesis1, EntryOUUThesis2, EntryOUU}. A similar approach to generating robust solutions based on the sensitivity matrix \cite{Desensitized}, rather than the covariance matrix, was presented in Ref.~\cite{MarsEntryDesensitized}. 
The works most closely related to our approach are Ref.~\cite{AltitudeUnderUncertainty}, where the objective was to maximize a function of the mean and standard deviation of altitude, and Ref.~\cite{MarsEntryDesensitized}, where the objective was to maximize terminal altitude while penalizing sensitivity terms related to variations in the initial state. Compared to Ref.~\cite{AltitudeUnderUncertainty}, our work differs by considering a multi-objective formulation that includes a downrange performance objective rather than strictly focusing on altitude, by considering closed-loop performance instead of open-loop, and by using the unscented transform instead of linear covariance propagation. Similarly, Ref.~\cite{MarsEntryDesensitized} also employed linear covariance propagation, but did consider closed-loop performance with fixed feedback gains. Their method made use of sensitivities, rather than statistics, in order to increase robustness to off-nominal situations. Additionally a single penalty factor was applied to all terminal state sensitivities, including flight path angle. In contrast, we penalize only altitude and downrange deviations, and use a separate penalty factor for each component. An additional difference is that we consider parametric uncertainty in the equations of motion rather than solely uncertainty in the initial state. 

%Reference~\cite{AltitudeUnderUncertainty} is perhaps the most closely related method to our work. The first important distinction is the multi-objective formulation in this work additionally considers the downrange standard deviation, rather than strictly focusing on altitude. Additional differences of our approach include the use of the unscented transform in place of linear covariance propagation, and the inclusion of the feedback control law to optimize closed-loop performance rather than open-loop performance. Reference~\cite{AltitudeUnderUncertainty} also considered path constraints on heat flux and dynamic pressure which are not considered in this work. 

%Optimal control under uncertainty adds an additional layer of computational complexity to the already difficult problem of solving optimal control problems. Using the UT results in a challenging optimal control problem in many state variables. In this work, the large-scale problem is solved using differential dynamic programming (DDP) \cite{DDP}. DDP is a shooting method originally devised to solve unconstrained nonlinear optimal control problems that has since seen numerous extensions, including to constrained problems \cite{DDP_ControlLimited,HDDP1,HDDP2,DDP_NonlinearConstraints,DDP_InteriorPoint}, and stochastic problems \cite{iLQG, DDP_Stochastic, ozaki_UT,ozaki2020tube}. 

%A common theme among all of these references is their application to robotics \cite{iLQG, DDP_Stochastic} or low thrust trajectory optimization \cite{HDDP1,HDDP2,ozaki_UT,ozaki2020tube}. Despite the apparent success in these fields, 
%Despite its apparent success in robotics and low-thrust trajectory optimization, DDP has not yet garnered much attention in the entry guidance community. 
%Simultaneous transcription methods, especially collocation methods, are popular for their ability to readily handle nonlinear constraints on states and controls, and indeed for deterministic problems with low state dimension, their applicability and effectiveness is undeniable. In a stochastic setting, however, the most common methods of converting the problem to a deterministic one involve an increase in the dimensionality of the state vector that render the problem difficult to solve this way. Meanwhile, DDP is an effective method for solving large-scale optimal control problems. 

%As with the MSL and Mars 2020 approach, the resulting entry guidance can be assessed by Monte Carlo simulations. We demonstrate that, by accounting for uncertainty in the design of the reference trajectory and bank angle profile, the Monte Carlo results are less likely to reveal unexpected performance and the need for further design iterations, in contrast to the MSL and Mars2020 approach. 
The remainder of the paper is organized as follows: the next section poses the entry guidance problem as a robust optimal control problem. The following section presents the unscented transform and differential dynamic programming algorithm used to solve the robust entry guidance problem. Next, a numerical assessment of the approach is presented, and the final section offers concluding remarks. 

\section*{Robust Optimal Entry Guidance Problem}
As stated earlier, we offer an alternative to the MSL and Mars 2020 range controller that explicitly considers an altitude objective in addition to range control. In common with that approach, we consider only the longitudinal motion. The entry guidance would be completed by adding lateral guidance, but this is not considered here except to consider qualitatively whether or not the longitudinal guidance leaves sufficient lateral control authority.
%TODO: Say that lateral guidance is not considered in the intro as well per Mease's suggestion 

The longitudinal equations of motion of a point-mass model of a Mars entry vehicle are written with respect to the planet-relative vehicle velocity magnitude, $v$, as the independent variable, and the state vector $\state_{\mathrm{lon}}=[h,\,\gamma,\, s]^T$ where $h$ is the vehicle altitude about the planet surface, $\gamma$ is the flight path angle, and $s$ is the downrange distance flown. The entry trajectory terminates at a fixed velocity $v_f$. Using velocity as the independent variable simplifies enforcing a constraint on the terminal velocity and allows the DDP algorithm, described in the next section, to be applied to the problem. The scalar control variable $u=\cos\sigma$ where $\sigma$ is the bank angle, and thus the longitudinal dynamics are
\begin{align}
h' &= \frac{v\sin\gamma}{-D - g\sin\gamma} \label{eq_dynamics_altitude}\\
s' &= \frac{v\cos\gamma}{-D - g\sin\gamma} \\
\gamma' &= \frac{\frac{L}{V}u + \left(\frac{v}{h+r_p}-\frac{g}{v}\right)\cos\gamma}{-D - g\sin\gamma} \label{eq_dynamics_fpa}
\end{align}
where $(\cdot)' = \frac{d\cdot}{dv}$, $r_p$ is the assumed constant planet radius, $g=\mu/(h+r_p)^2$ is the magnitude of the gravitational acceleration, and the lift and drag accelerations are
\begin{align}
D = \frac{1}{2}\frac{\rho}{\beta} v^2 \\
L = D(\frac{L}{D})
\end{align}
where $\rho=\rho_0\exp\left(-\frac{h}{h_s}\right)$ is an exponential model of the atmospheric density, $\beta=\frac{m}{C_DS}$ is the ballistic coefficient, $m$ is the vehicle mass, $L/D$ is the lift-to-drag ratio, and $C_D$ is the drag coefficient.
%TODO: Give values of rho0 and hs?

In designing the reference trajectory, the effects of feedback terms in the control are accounted for. Thus the control $u$ consists of both a reference control $\ur$, and a feedback term $\delta u$, in contrast to open-loop design methods where $u=\ur$. 
The control law is assumed to be to a saturated linear state feedback 
\begin{align}
u(v,\state_{\mathrm{lon}}) &= \mathrm{sat}_{[0,1]}\left(\frac{\frac{L}{D}_{\mathrm{ref}}\ur(v)}{\frac{L}{D}} + \delta u(v,\state_{\mathrm{lon}})\right) \label{eq_control}\\
\delta u &= k_D\delta D + k_{\gamma}\delta\gamma + k_s\delta s \label{eq_feedback}
\end{align}
%TODO: L/D control, reference Eq.3 in MSL design paper, talk about the "strange" form of the open loop component
where we note that, consistent with state-of-the-practice EDL operations on Mars, drag acceleration has been used as a feedback term in place of altitude. 
The gains $k_i$ in Eq.~(\ref{eq_feedback}) may be functions of velocity. 
In principle the feedback control law used for MSL and M2020 could be adopted, but it would require computing influence coefficients by backward integration at each iteration of solving the optimal control problem. By using the feedback control in Eq.~(\ref{eq_feedback}), the computation is reduced significantly in line with one of our primary objectives, and we will demonstrate that good guidance performance can be achieved.
The saturation function is defined
\begin{align*}
\mathrm{sat}_{[a,b]}(x) = \left\{\begin{array}{lc}
        a, &  x < a\\
        x, &  a\le x \le b\\
        b, &  b < x
        \end{array} \right. % The period stops a warning about not closing the left 
\end{align*}
The saturation function is required to ensure that, regardless of the value of the reference control \ur, the feedback control $u(v,\state)$ always satisfies the control limits. Because the control variable is the cosine of an angle, its magnitude must be bounded by one. Due to the low lift capability of current generation Mars entry capsules, it may be prudent to further restrict the vehicle from bank angles that orient the lift vector downward, so in our application we apply
\begin{align}
	0 \le \ur(v) \le 1 \label{eq_control_bounds}
\end{align}
which disallows bank angle magnitudes greater than $90^\circ$.
The state deviations in Eq.~(\ref{eq_feedback}) are computed with respect to the reference state at the current velocity, e.g., $\delta D(v) = D(v) - D_{\mathrm{ref}}(v)$.
The form of Eq.~(\ref{eq_control}) may be unfamiliar but notice that when the control is not saturated, Eq.~(\ref{eq_control}) may be rearranged as
\begin{align}
	\frac{L}{D}u(v,\state_{\mathrm{lon}}) &= \frac{L}{D}_{\mathrm{ref}}\ur(v) + \frac{L}{D}\delta u(v,\state_{\mathrm{lon}}) \label{eq_control_rearranged}
\end{align}
Compared with the MSL approach (see Eq.~(2) in Ref.~\cite{MSL_EDL2}), the only difference lies only in the feedback terms $\delta u$. Just like the ETPC, the control in Eq.~(\ref{eq_control}) commands the reference vertical $ L/D $, rather than a reference fraction of the available $ L/D $, which turns out to be more robust choice in the presence of aerodynamic uncertainty.

The stress trajectories used in the MSL design were identified by considering worst-on-worst dispersion factors influencing downrange accuracy and altitude\cite{MSL_EDL2}; these factors include entry flight path angle errors, and multipliers on $L/D$ ratio, ballistic coefficient, and atmospheric density. Thus, in this work, uncertainty in $\param^T = [L/D,\,\beta,\rho_0]$ is modeled in addition to the initial state uncertainty. 

As a result of considering uncertainty, the state variables are random variables and the goal is to optimize a distribution of trajectories rather than a single trajectory. The mean, or expected value, of a random variable, $x$ is denoted by $\bar{x}=\E{x}$, and its standard deviation by $\sigma_{x}$. A nominal trajectory is one in which all uncertain quantities are set to their mean value. For nonlinear systems, including entry dynamics, the nominal trajectory is in general not equal to the mean trajectory except at the initial time. The initial state vector is assumed to be governed by a distribution with known mean .
In this work, we assume the initial state is normally distributed, expressed as 
\begin{align}
\state_{\mathrm{lon}}(v_0)\sim N(\bar{\state}_0,\,\cov_{\state_0})\label{eq_IC},
\end{align}
where $\bar{\state}_0\in \mathbb{R}^{3}$ is the mean initial state, and $\cov_{\state_0} \in \mathbb{R}^{3\times3}$ is the initial state covariance matrix.
but the approach also applies to non-normal distributions. Even if the initial state is normally distributed, in general the state distribution will evolve into a non-normal distribution under the nonlinear entry dynamics. The parameter variations are constants applied such that
\begin{align}
	\param(v,\state) &= \bar{\param}(v,\state) + \delta\param\sigma_{\param}(v,\state) \label{eq_param_unc}\\
	\delta\param &\sim N(\mathbf{0},\cov_{\param})
\end{align}
where $\cov_{\param}\in \mathbb{R}^{3\times3}$ is the parameter covariance matrix.
This formulation allows both the mean and standard deviation of the parameters to vary along the trajectory, and assumes that a constant fraction of the standard deviation is applied throughout the trajectory. This could be used to represent, for example, an atmospheric density model in which the mean and standard deviation vary with altitude, such as MarsGRAM \cite{MarsGRAM2010}.
Equation~(\ref{eq_param_unc}) can also be used to represent uncertainties given as constant fractions of the mean by choosing $\sigma_{\param}(v,\state) = \bar{\param}(v,\state)$, which simplifies to
\begin{align}
		\param(v,\state) &= \bar{\param}(v,\state)(1 + \delta\param)
\end{align}
An example where this is useful is the $L/D$ profile. The $L/D$ profile depends on the angle of attack profile, which is generally considered to be a given function of Mach number for Mars entry guidance. Regardless of the mean profile, uncertainties in the aerodynamic coefficients are often given as percentages of the mean. More generally, any model in which each uncertain parameter may be modeled as a function of a constant scalar value can treated without alteration of the solution method presented in the following section. 
For any such model, the uncertain parameters are accounted for by considering an augmented state vector, $\state\in \mathbb{R}^{6}$, formed by appending the parameter variations to the longitudinal state vector
\begin{align}
	\state = \left[
	\begin{matrix}
		\state_{\mathrm{lon}} \\
		\delta\param
	\end{matrix}
	\right]
\end{align} 
and augmenting the entry dynamics with trivial dynamics
\begin{align}
	\state' = \left[
	\begin{matrix}
		\state'_{\mathrm{lon}} \\
		\mathbf{0}
	\end{matrix}
	\right]
\end{align}

The robust optimal guidance problem is to determine the piece-wise constant reference control $\ur\left[v_0,v_f\right]$ that minimizes the objective functional
\begin{align}
%J &= -\E{h(v_f)} + w_h\V{h(v_f)}^{\frac{1}{2}} + w_s\V{s(v_f)}^{\frac{1}{2}}.
%J(\ur\left[v_0,v_f\right]) &= -\E{h(v_f)} + w_h\sigma_h(v_f) + w_s\sigma_s(v_f) \label{eq_objective}
J &= -\bar{h}(v_f) + w_h\sigma_h(v_f) + w_s\sigma_s(v_f) \label{eq_objective}
\end{align}
subject to the equations of motion $(\ref{eq_dynamics_altitude})-(\ref{eq_dynamics_fpa})$, the uncertain initial state~(\ref{eq_IC}), and the control bounds (\ref{eq_control_bounds}). 
The objective consists of maximizing mean altitude and minimizing standard deviation of altitude and downrange distance. Large values of the weights $w_h\ge0$, $w_s\ge0$ result in more robust solutions at the expense of mean altitude performance.
%The low end of the altitude distribution is important because sufficient altitude is required to provide time for subsequent landing operations.
%Thus the weight $w_h\ge0$ allows for additional emphasis on maximizing a measure of the low end of the distribution. Additionally, while the mean downrange distance is unconstrained, the weight $w_s\ge0$ is used to discourage solutions from sacrificing downrange accuracy for small altitude improvements. 
The mean downrange distance is unconstrained, even when considering a fixed target location, with the understanding that in the mission design phase the entry interface point can be moved to accommodate the optimal downrange distance. While heat flux and acceleration load are important considerations in mission planning, past studies have concluded that the entry flight path angle is a first-order driver of these quantities, while the control profile has only a secondary effect \cite{MSL_EDL2}. As a result, these constraints are handled by proper selection of the mean entry flight path angle, rather than by including them explicitly in the problem formulation. 

The robust optimal guidance solution consists of the reference control and the reference trajectory, both of which are stored for interpolation. The reference trajectory is defined to be the mean trajectory, rather than the nominal trajectory. If the UT estimates the mean downrange distance accurately, then the mean error should be very nearly zero in the Monte Carlo assessment results. 

% TODO: Reference Mars Entry Bank Profile Opt for Terminal State defines an EFPA lower bound (steepest) by considering full lift up profiles (do these maximize accel loads?)
Either variances or standard deviations may be used in Eq.~(\ref{eq_objective}). In practice, appropriate choices of the weights can produce equivalent results, but standard deviations are preferred because they are in the same units as the state variables, and allow for more natural interpretations of the weights. An additional consideration is that the square root is not differentiable at zero, but both in theory and in practice, the terminal standard deviations are never exactly zero since the covariance matrix is positive definite.

\section*{Robust Optimal Control Solution via Unscented Transform and DDP}
A key issue in solving the robust optimal guidance problem is computing the expected values and standard deviations in the objective functional and feedback terms. Broadly speaking, uncertainty quantification (UQ) methods trade between accuracy and the amount of computation required. For example, linear covariance propagation is one of the most efficient UQ methods for computing the first two probability moments, but its accuracy depends on the nonlinearity of the system dynamics. At the other extreme, Monte Carlo simulation can estimate higher order moments to arbitrary accuracy, but may require a huge number of samples in order to do so. Since UQ will be performed at each optimal control solver iteration, the method chosen must strike a careful balance these two aspects. For very fast but inaccurate methods, the solution may not perform as expected in a higher fidelity UQ, such as Monte Carlo simulation, and the benefit of the approach may be diminished or lost entirely. On the other hand, accurate methods will result in very long solution times. In this work, the unscented transform (UT) \cite{UT1997} is chosen to compute the required statistics. The UT is preferable to linear covariance techniques because the sigma points, described in the following subsection, are propagated through the full nonlinear, saturated dynamics, and are thus able to capture their effects on the distribution more accurately than linearization. 
%While random sampling can produce more accurate estimates than either linear propagation or the unscented transform,
%It is also preferable to random sampling because the number of sigma points is generally lower than the number of samples required for equivalent precision, and the sigma points are given deterministically by the UT algorithm. %TODO: citation?

%TODO: Talk about how we're taking the method but with a modification to the method and also the formulation to make it applicable
Next, because control saturation is a key issue in the closed-loop dynamics, the control-limited box-DDP algorithm~\cite{DDP_ControlLimited} is employed to solve for the optimal control. While this approach results in reference controls that satisfy the control limits, the saturation function in Eq.~(\ref{eq_control}) is required in the forward integration to ensure the closed-loop sigma point trajectories also respect the control limits.

\subsection*{Unscented Transform}
The unscented transform is a method to approximate the first two moments of a nonlinear transformation of a probability distribution. Consider a scalar quantity $q\in\mathbb{R}$ resulting from a nonlinear transformation $q = F(\param)$ of a vector $\param\in\mathbb{R}^n$ with known mean $ \bar{\param} $ and covariance $ \cov_{\param} $. A set of $2n+1$ sigma points and associated weights are computed 
\begin{align*}
\param_0 &= \bar{\param} \\
\param_i &=  \bar{\param} + \left(\sqrt{\alpha \cov_{\param}}\right)_i,\, \,i=1,...,n \\
\param_{i+n} &=  \bar{\param} - \left(\sqrt{\alpha\cov_{\param}}\right)_i, \, \,i=1,...,n\\
w_0 &= \frac{\alpha - n}{\alpha} \\
w_i &= w_{i+n} = \frac{1}{2\alpha}, \, \,i=1,...,n
\end{align*}
where $\alpha$ is a scaling parameter, and $\left(\sqrt{\alpha \cov_{\param}}\right)_i$ is the $i^{\mathrm{th}}$ column of the matrix square root of $\alpha \cov_{\param}$. The sigma points are then mapped through the transformation
\begin{align}
q_i = F(\param_i),\;\;i=0,...,2n
\end{align}
and finally, the mean and variance are estimated using the weights and transformed sigma points
\begin{align*}
\bar{q} &\approx \sum_{i=0}^{2n}w_iq_i\\
\sigma_{q} &\approx \left(\sum_{i=0}^{2n}w_i\left(q_i - \bar{q}\right)^2\right)^{\frac{1}{2}}
\end{align*}
In our application, the initial distribution comprises the augmented state vector, including parametric uncertainty, the nonlinear transformation is the integration of the longitudinal entry dynamics to the terminal velocity, and the quantities of interest are downrange distance and altitude. 

When applying the unscented transform, the scaling parameter $\alpha$ is important in precisely estimating the statistics. For any value of $\alpha$, the sigma point distribution has the same mean and covariance as the initial distribution. Increasing the value of $\alpha$ places the sigma points $\param_i$ further from the nominal sigma point $\param_0$ and reduces their weight. Using a small value of $\alpha$ results in sigma points with only small deviations from the nominal, and for the entry guidance problem in this paper, the effects of the controller and saturation nonlinearity will not be accurately quantified. In our numerical studies, it appears that no single value of $\alpha$ minimizes estimation errors for all control profiles. However, it is important to recall that estimating statistics is not the purpose of the proposed trajectory optimization. So long as the reference trajectories designed using the UT confer benefits in Monte Carlo simulation, the UT statistics are sufficiently accurate, as will be demonstrated by the results of the numerical assessment. 
%TODO: Maybe word last sentence better?

\subsection*{Control-Limited Differential Dynamic Programming}
In this subsection we review the differential dynamic programming method, proposed in Ref.~\cite{DDP_ControlLimited}, that will be used to solve the optimal control problem. We also propose a simplification that vastly reduces computational time and memory requirements at the expense of lower convergence rate. This simplification is instrumental in solving the large-scale problem.
The algorithm is formulated in discrete time, so the continuous time dynamics must be discretized. In our numerical examples, Euler integration is used:
\begin{align}
\state_{i+1} = \mathbf{f}(v, \state_i,\control_i) = \state_i + \state_i'\Delta v \label{eq_discrete_dynamics}
\end{align}
A trajectory $\{\State,\Control\}$ is a sequence of controls $ \Control=\{\control_0,\control_1,...,\control_{N-1}\} $ and corresponding states $\State=\{\state_0,\state_1,...,\state_N\}$ determined by integrating (\ref{eq_discrete_dynamics}) from $\state_0$.
Although the optimal control objective as posed considers only a terminal cost, in this section we consider a generic cost function $J$ consisting of a sum of running costs $l$ and a terminal cost $l_N$:
\begin{align}
J(\state_0,\Control) = \sum_{i=0}^{N-1}l(\state_i,\control_i) + l_N(\state_N)
\end{align}
Let $\Control_i$ be the tail of the control sequence, $\{\control_i,\control_{i+1},...,\control_{N-1}\}$, and the cost-to-go $J_i$, defined as the partial sum of costs from $i$ to $N$ is
\begin{align}
J_i(\state,\Control_i) = \sum_{j=i}^{N-1}l(\state_j,\control_j) + l_N(\state_N)
\end{align}
The value function at timestep $i$ is the optimal cost-to-go at \state
\begin{align}
V_i(\state) = \min_{\Control_i} J(\state, \Control_i)
\end{align}
and at the final timestep the value function is equal to the terminal objective. The dynamic programming principle reduces the problem of minimization over $\Control_i$ to a sequence of minimization problems over $u$ at each timestep 
\begin{align}
V(\state) = \min_{\control}\left[l(\state,\control) + V^+(\mathbf{f}(\state,\control))\right] \label{eq_dynamic_programming}
\end{align}
where $V^+$ is the value at the next time step.
Let the pseudo-Hamiltonian $Q(\delta\state,\delta\control)$ be the change in value function as a function of perturbations to the pair $(\state,\control)$:
\begin{align}
Q(\delta\state,\delta\control) = l(\state+\delta\state,\control+\delta\control) + V^+(\mathbf{f}(\state+\delta\state,\control+\delta\control))
\end{align}
The second-order expansion of $ Q $ is given by
\begin{align}
Q_\state &= l_\state + \mathbf{f}_\state^T V^+_\state \\
Q_\control &= l_\control + \mathbf{f}_\control^T V^+_\state \\
Q_{\state\state} &= l_{\state\state} + \mathbf{f}_\state^T V^+_{\state\state}\mathbf{f}_\state + V^+_\state \mathbf{f}_{\state\state} \label{eq_hessian1}\\
Q_{\control\state} &= l_{\control\state} + \mathbf{f}_\control^T V^+_{\state\state}\mathbf{f}_\state + V^+_\state \mathbf{f}_{\control\state} \label{eq_hessian2}\\
Q_{\control\control} &= l_{\control\control} + \mathbf{f}_\control^T V^+_{\state\state}\mathbf{f}_\control + V^+_\state \mathbf{f}_{\control\control} +\lambda I \label{eq_hessian3}
\end{align}
where the subscripts denote partial derivatives with respect to that quantity, and the final term in each of the Hessian equations (\ref{eq_hessian1})-(\ref{eq_hessian3}) are tensor-vector contractions, and $ \lambda $ is a regularization parameter.  %TODO: rewrite them as sums over the 'pages' of the hessian

The second derivatives $\mathbf{f}_{\state\state},\, \mathbf{f}_{\control\state}$ in Eqs.~(\ref{eq_hessian1}) and (\ref{eq_hessian2}) are $n\times n\times n$ and $m\times n\times n$ tensors, respectively, that must either be stored at $N$ timesteps, or else recomputed in the event the backward pass must be repeated with increased regularization. For large $n$, both computing and storing these quantities is problematic. Unfortunately, removing these terms entirely, as is done in the iterative Linear Quadratic Gaussian method \cite{iLQG}, leads to poor performance for the problem under consideration. In particular, the first-order algorithm has a noticeably smaller radius of convergence, and different guesses at the initial control sequence often lead to different local minima. In contrast, the full second-order DDP algorithm consistently converges to a single minimum from most initial guesses. Thus, we propose a compromise in which only the $n\times m \times m$ tensor $f_{\control\control}$ is retained in the above equations. Numerical experiments suggest this is sufficient to avoid poor solutions, at the expense of a slower convergence rate than the full algorithm with all second-order terms included. However, in the situation at hand, in which the number of controls $m$ is fixed and small ($ m=1 $), and the augmented state dimension, which depends on the number of uncertainties under consideration, is large ($n=78$ for six uncertainties), this modification proves to be enabling for $N\geq 100$. A limited-memory version of the Quasi-Newton approximations to these terms proposed in \cite{QNDDP} may also be a good choice to obtain superlinear convergence properties of the approximated second-order algorithm while reducing the memory required. 
%(This reduces complexity from cubic in $n$ to linear)

The optimal control modification $\delta\control^*$ for some perturbation $\delta\state$ is a locally-linear feedback control $\delta\control^* = \mathbf{k} + K\delta\state$ obtained by minimizing the quadratic model subject to linear bounds on the controls
\begin{align}
\mathbf{k} = &\arg\min_{\delta\control} Q(\delta\state,\delta\control) \\
&\mathrm{subject\,to\,\;} \nonumber\\
\control_{\min}\le &\control+\delta\control \le\control_{\max}
\end{align}
and $K = -Q_{\control\control}^{-1}Q_{\control\state}$. Substituting the optimal control into the expansion of $Q$, a quadratic model of $V$ is obtained with derivatives
\begin{align}
\begin{split}
\label{eq_value_recurse}
V_\state &= Q_{\state}- K^TQ_{\control\control}\mathbf{k}\\
V_{\state\state} &= Q_{\state\state} - K^TQ_{\control\control}K.
\end{split}
\end{align}
The backward pass is performed by initializing $V$ and its derivatives with the value and derivatives of the terminal objective, then recursively computing the optimal control policy and Eq.~(\ref{eq_value_recurse}).

The forward pass uses the newly computed control policy to integrate the new trajectory, subject to a backtracking line search
\begin{align}
\hat{\state}_0 &= \state_0 \\
\hat{\control}_{i} &= \control_i + \epsilon \mathbf{k}_i + K_i(\hat{\state}_i - \state_i)\\
\hat{\state}_0 &= f(\hat{\state}_i,\hat{\control}_i)
\end{align}
where $\epsilon$ is a search parameter initialized to 1 and reduced until the value function shows improvement. The backward-forward iterations are repeated until convergence to a locally optimal trajectory. In the event the backward pass fails to produce a descent direction, the regularization parameter $\lambda$ is increased and the backward pass is repeated. See Ref.~\cite{DDP_ControlLimited} for more details about the line search and regularization procedures. 
%
\subsection*{Objective Reformulation}
In order for DDP to be applied to our robust optimal control problem, it is essential to reformulate some or all of the objective terms as running costs rather than terminal costs. This is because the terminal formulation results in singularity of the Hessian of the pseudo-Hamiltonian with respect to the control, and this quantity must be invertible for the algorithm to proceed. In practice, one could also add a small control regularization term to the objective, but experience indicates that reformulating the mean altitude objective is preferable. Due to linearity of the expectation operator, we have the following relationships for the time rate of change of the first two moments
\begin{align}
\frac{d }{d t}\E{x} &= \E{\dot{x}} \\
\frac{d }{d t}\V{x} &= \E{2x\dot{x}} - 2\E{x}\E{\dot{x}} \\
\sigma_x &= \V{x}^{\frac{1}{2}} \\
\frac{d }{d t}\sigma_x &= \frac{1}{2\sigma_x}\frac{d }{d t}\V{x}
%\frac{d }{d t}\V{x}^{\frac{1}{2}} &= \frac{1}{2\V{x}^{\frac{1}{2}}}\frac{d }{d t}\V{x}
\end{align}
and thus the running cost used in the reformulated objective function is 
\begin{align}
%J = \int_{v_0}^{v_f}-\E{h'} +  2w_h(\E{hh'}-\E{h}\E{h'}) + 2w_s(\E{ss'}-\E{s}\E{s'})\mathrm{d}v \\
%J = \sum_{i=0}^{N-1} \left[-\E{h'} +  2w_h(\E{hh'}-\E{h}\E{h'}) + 2w_s(\E{ss'}-\E{s}\E{s'})\right]\delta v 
l(\state,\control) = -\E{h'} +  \frac{w_h}{\sigma_h}(\E{hh'}-\E{h}\E{h'}) + \frac{w_s}{\sigma_s}(\E{ss'}-\E{s}\E{s'})
\end{align}
While reformulating the mean altitude objective is essential to solving the problem, especially when $w_s=w_h=0$, reformulating the standard deviation terms is not. However, in numerical experiments we found that while both formulations result in the same solution, using running costs for the standard deviations allows DDP to converge in noticeably fewer iteration. Running costs are used in the results in this paper.

The non-differentiability of the saturation function in Eq.~(\ref{eq_control}) at the limits produces numerical difficulties for the DDP algorithm, and thus we substitute the following smooth approximation when solving the optimal control problem
\begin{align*}
\mathrm{sat}_{[0,1]}(x) \approx \frac{1}{2} + \frac{1}{4K}\log\left(\frac{\cosh (2Kx)}{\cosh (2K(x-1))}\right) 
\end{align*}
where $K>0$ is a tuning parameter. Larger values of $K$ reduce approximation error but increase the derivative magnitude at the saturation limits. $K=20$ reduces the maximum error to less than $1\%$ and is used in our implementation.
%\subsubsection*{Remark}
%Prior to solving the challenging optimal control problem under uncertainty, we applied DDP to altitude-optimal control without considering uncertainty. This was done so that we could verify it recovers the optimal bang-bang solutions, which could also be computed using a different method such as GPOPS-II\cite{GPOPS2}.

%In addition to eliminating a terminal constraint, using velocity as the independent variable was also advantageous in solving the optimal control problem via DDP. Although we have not done a full analysis, one possibility is that, because DDP respects the temporal structure of the optimal control solution, this removes derivatives with respect to velocity. Because the aerodynamic accelerations are quadratic in velocity, this may have a large impact on the problem sensitivity. We also examined downrange distance as the independent variable, and found that convergence was poor unless a very close guess at the optimal control was used. In contrast to velocity, downrange distance does not appear in the equations of motion. 

\section*{Guidance Assessment}
A series of numerical experiments is conducted to assess the robust optimal guidance. In the first subsection, reference trajectories and controls generated by our approach are compared to those designed by nominal optimal control with fixed bank angle margins. By nominal optimal control we refer to the optimization of a single trajectory without considering uncertainty and closed-loop guidance. The second subsection investigates how the optimal trajectories change with the weights $w_h$ and $w_s$, incorporating both initial state uncertainty and parametric uncertainty. The third subsection presents a detailed look at one solution and the corresponding Monte Carlo statistics.

All simulations are conducted for a five metric ton entry vehicle with a nominal $L/D=0.28$, i.e., in the same class as the Sample Retrieval Lander considered for the Mars Sample Return campaign \cite{MSR}. 
In order to contend with the issue that the entry velocity, our chosen independent variable, is not generally monotonically decreasing from the entry interface, 
the simulations begin at a lower altitude where the atmosphere is sufficiently dense for bank angle control to be effective and the vehicle velocity magnitude is strictly decreasing. This is similar to the way MSL and Mars 2020 flew a pre-bank angle until active range control began. The mean entry state, used in all of the experiments, is $x_0 = [54.5\,\mathrm{km},\,-11.5^{\circ}, 0\,\mathrm{km}]^T$ at $v_0 = 5525$ m/s. The initial state uncertainty around the mean is $[\sigma_h, \sigma_{\gamma}, \sigma_s ] = [2.5 \mathrm{km},\, 0.25^{\circ},\, 10\mathrm{ km}]$. Uncertainty is also modeled in the $L/D$ ratio, ballistic coefficient, and atmospheric density with $\sigma_{L/D} = 5\%$, $\sigma_{\beta} = 5\%$, $\sigma_{\rho} = 6.66\%$. With uncertainty in each state variable as well as in three model parameters, there are six uncertainties under consideration and the unscented transform yields 13 sigma points, resulting in an augmented state vector with 78 state variables. The terminal entry velocity is $v_f=460$ m/s. The problem is discretized into $N=250$ fixed velocity increments, and the control is assumed to be constant on each interval. Four Euler integration steps are taken over each velocity step. The value of $ N $ and the number of integration steps were chosen by validating the terminal altitude and downrange distance against adaptive stepsize integration using MATLAB's \textit{ode45} function. The acceptable error tolerance across all sigma points was 100 m. The unscented transform tuning parameter $\alpha=15$ in all examples. This value approximately minimizes the UT-estimated downrange distance standard deviation for our guess trajectory, i.e., the trajectory with which DDP is initialized. 

\subsection*{Reference Trajectory Comparison}
In this subsection we compare the performance of the robust optimal guidance using six different reference trajectory-control pairs. For all six cases, the controller, Eq.~(\ref{eq_control}), is used with the same feedback gains and feedback control limits, Eq.~(\ref{eq_control_bounds}). In the first three cases, a reference trajectory and control are generated by solving the optimal control problem $\min J = -h(v_f)$ subject to nominal longitudinal dynamics with different limits on the reference control to preserve varying amounts of margin for feedback. For the remaining three cases, the method proposed in this paper is used to generate the reference trajectory and control. Monte Carlo simulations are used to assess the guidance performance.

Case 1 considers $u_{\mathrm{ref}}\in[0,1] = [\cos90^{\circ},\cos0^{\circ}]$, Case 2 considers $u_{\mathrm{ref}}\in [\cos75^{\circ},\cos15^{\circ}]$, and Case 3 considers $u_{\mathrm{ref}}\in[0,1] = [\cos60^{\circ},\cos30^{\circ}]$.
Bank angle margin is important for range control, as well as for preserving control authority for lateral guidance, which is not modeled here. 
Cases 4-6 solve the robust optimal guidance problem with different choices of the weights to design the reference trajectories. Case 4 optimizes mean altitude ($ w_h=w_s=0 $), Case 5 optimizes the 3$\sigma$-low altitude ($ w_h=3,\,w_s=0 $), and Case 6 optimizes a combination of mean altitude and 3$\sigma$ downrange deviation ($ w_h=0, w_s=3 $). Recall that each of the six cases may target a different downrange distance.

The feedback gains are constants $[k_D, k_{\gamma}, k_s] = [0.0725, -0.025, -0.004]$.  These gains were computed by minimizing the UT-estimated downrange distance standard deviation for the initial guess at the control profile, which is a linear (in velocity) ramp from zero to one. Different initial guesses for the optimal profile would yield some variation in these gains, and superior results can be obtained by optimizing the gains for each solution, but using the same gains for all of the cases allows us to highlight the impact of the reference trajectory without confounding variables. These gains are used in the optimization process for cases 4-6, and in the Monte Carlo simulations for all six cases. 

%TODO: Talk about the Monte Carlo setup, maybe help distinguish between optimization and simulation 
For each case, a Monte Carlo simulation with 2000 sample trajectories is conducted. The samples are drawn using Latin Hypercube sampling. The bank angle command is updated at a rate of 1 Hz. Range error is the distance between the terminal downrange distance defined by the reference trajectory and the downrange distance flown. The primary metrics we will examine are the 3$\sigma$-low altitude $=\bar{h}-3\sigma_h$ and 3$\sigma$ range error $= 3\sigma_s$ at the final velocity. The terminal state distributions are generally non-Gaussian, and in practice percentiles are used to specify mission requirements. However, metrics based on the mean and standard deviation are presented, because these are the values estimated by the unscented transform, which allows for a direct comparison with the Monte Carlo results.
\begin{figure}[h!]
	\centering
	\includegraphics[width=1\textwidth]{ddp/comparison_controls}
	\caption{The profiles from deterministic optimal control (left) exhibit the well known bang-bang structure of altitude optimal trajectories. In comparison, the robust control histories (right) exhibit significantly behavior depending on the weights applied.}
	\label{fig_control_comparison}
\end{figure}

Figure~\ref{fig_control_comparison} shows the resulting control profiles for the six cases. The altitude-optimal controls resulting from nominal optimal control (Cases 1-3) exhibit the well known bang-bang profile, while the robust optimal control profiles are quite varied depending on the weights selected. For Case 4, which optimizes mean altitude, the profile is somewhat similar to the bang-bang profiles, but retains margin at high velocities and has a slower transition to a lift up ($u=1$) orientation. An interesting feature of the robust profiles is that saturation still occurs at some velocities. This suggests that the optimal amount of margin is a quantity that varies along the trajectory, which implies using a fixed margin will produce inferior results as the results presented next indicate. Although lateral guidance control authority ($L/D \sin\sigma$) is not considered in the formulation of the robust performance optimization problem, the solutions show that there is significant, though non-uniform, margin. Further consideration of compatibility with lateral guidance is left as future work. %outside the scope of this paper.

%TODO: Add a note that the target downrange distance is different for each Case.

The results are summarized in Tables~\ref{table_deterministic}-\ref{table_robust}. Case 1 preserves no margin at all, and as a result has the highest nominal terminal altitude of the three cases. This translates into the highest mean altitude of the three cases, despite the lack of margin. However, the lack of margin causes poor range control performance in many trajectories, leading to large mean and 3$\sigma$ range errors. While the mean error can be reduced by target biasing, the footprint is unacceptably large. As the amount of control margin increases in Cases 2 and 3, both the mean and 3$\sigma$ range errors drop dramatically while the mean altitude also decreases. The 3$\sigma$-low altitudes display non-monotonic behavior with respect to bank margin because the mean altitude loss may or may not be offset by the reduction in altitude deviation. This is true for Case 2, which has a higher 3$\sigma$-low altitude relative to Case 1. In Case 3, the drop in mean altitude is greater than the drop in the 3$\sigma$-low altitude because the altitude deviation has decreased further. 

Case 4 ($ w_h=w_s=0 $) optimizes the mean altitude with knowledge of the problem uncertainty and guidance gains. The mean altitude is 50 m lower than Case 1 because the UT overestimated the mean. However, this difference is quite small, and the performance is more robust because the 3$\sigma$-low altitude is higher and the 3$\sigma$ range error is smaller. Case 5 ($ w_h=3,w_s=0 $) adds additional weight to the altitude standard deviations, resulting in a 500 m increase to the 3$\sigma$-low altitude despite a 300 meter drop in mean altitude relative to Case 4. Case 5 has the highest 3$\sigma$-low altitude of the six cases, with a 200 m increase over the best of the three profiles designed without considering uncertainty. Case 6 ($ w_h=0, w_s=3$) places no weight on altitude standard deviation, but weights range errors heavily, and produces a 1.5 km decrease in 3$\sigma$ range errors as a result. It has the lowest range error of the six cases, and compared to the large margins of case 3, still produces a higher 3$\sigma$-low altitude.

This demonstrates the benefit of robust optimal control to designing reference trajectories. Additionally, the flexibility of the weights allows the trajectory designer to trade between range errors and altitude performance to the extent that the reference trajectory can alter closed-loop performance. 
\begin{table}[h!]
	\centering
	\caption{Monte Carlo statistics for cases 1-3, with reference trajectories designed using optimal control and fixed bank margins.}
	\includegraphics[width=0.65\textwidth]{ddp/table_deterministic}
	\label{table_deterministic}
\end{table}
\begin{table}[h!]
	\centering
	\caption{The Monte Carlo statistics for cases 4-6, with reference trajectories designed using the proposed method.}
	\includegraphics[width=0.65\textwidth]{ddp/table_robust} %
	\label{table_robust}
\end{table}
%Figures~\ref{fig_robust_alt}-\ref{fig_robust_range} show the estimated $3\sigma$ deviations around the mean as well as the sigma point trajectories for scenario 1.
%\begin{figure}[h!]
%	\centering
%	\includegraphics[width=1\textwidth]{ddp/matlab/NominalDrag}
%	\caption{Reference drag profile for each of the four scenarios in Table~\ref{table_comparison}.}
%	\label{fig_drag}
%\end{figure}
%\begin{figure}[h!]
%	\centering
%	\includegraphics[width=1\textwidth]{ddp/matlab/RobustTrajAlt}
%	\caption{Dispersed altitude performance for scenario 1.}
%	\label{fig_robust_alt}
%\end{figure}
%\begin{figure}[h!]
%	\centering
%	\includegraphics[width=1\textwidth]{ddp/matlab/RobustTrajRange}
%	\caption{Dispersed range performance for scenario 1.}
%	\label{fig_robust_range}
%\end{figure}


\subsection*{Standard Deviation Weighting Study}
\begin{figure}[h!]
	\centering
	\includegraphics[width=1\textwidth]{ddp/python/WeightSweepMC}
	\caption{Contours of the Monte Carlo 3$\sigma$ low altitude (left) and 3$\sigma$ downrange distance error (right) as a function of the weights $w_h$ and $w_s$.}
	\label{fig_weight_sweep}
\end{figure}
%TODO: Double check these numbers, they may have changed 
From the previous subsection it is evident there exists a design space where weights on deviations produce superior results to simply optimizing the mean altitude when tail behavior of the distribution is important. This section investigates the extent to which weighting the standard deviations can alter the trajectory performance. The robust optimal control problem is solved for a variety of weights. The results are summarized in Fig.~\ref{fig_weight_sweep}. Again the advantages of weighting the standard deviations compared with optimizing only the mean are made clear. Comparing $w_h=w_s=0$ with, e.g., $w_h=1,\,w_s = 1$ demonstrates that a 50\% reduction in the standard deviation of downrange distance can be obtained with no loss in the low end of the terminal altitude distribution. Additionally, we can numerically determine the trade between terminal altitude and downrange robustness. The minimum downrange standard deviation achieved ($w_h=1,w_s=3$) was 0.7 km, an 85\% reduction from the mean optimization with $\sigma_s(v_f)=5.1$ km. These trajectories show clear improvements with no change to the guidance gains - simply a different reference trajectory is used to improve the results. The downrange distance shows only small improvement for weights $w_s>1$, but the low altitude continues to drop as this weight is increased. Thus, a compromise such as $w_h=w_s=1$ strikes a good balance between the two objectives. 

These results also show some inaccuracy due to using the unscented transform. The 3$\sigma$-low altitude should be maximized when the objective function is exactly this quantity, i.e., $w_h=3,\,w_s=0$, but due to small estimation errors this is not the case, and the highest 3$\sigma$-low altitude is achieved with $w_h=2,\,w_s=0$. Similarly, the 3$\sigma$ range error is not minimized when $w_h=0,\,w_s=3$, but instead $w_h=1,\,w_s=3$. However, the difference in each of these cases is quite small. This issue is related to that fact that a constant UT scaling parameter $\alpha=15$ is used in all computations. Our experience with the numerical results suggests that the optimal value of $\alpha$ depends on the weights selected, and larger weights should use a higher $\alpha$. 
%Nevertheless, the solutions at all weight values are acceptable
%This also highlights the power of the approach as a design tool. If, for example, the $3\sigma$-low terminal altitude is not sufficient for any values of $w_h,\,w_s$, then some aspect of the mission must be reconsidered: increase the terminal velocity, increase the vehicle $L/D$, decrease ballistic coefficient, etc. If a $3\sigma$-low altitude limit is known, then only points that satisfy this limit are considered, and the point with the lowest downrange error may be selected. 

These results used constant feedback gains, so it is likely that methods of generating gains that vary along the trajectory, such as the Linear-Quadratic Regulator, Apollo influence coefficients \cite{Apollo}, or joint optimization of the feedback gains in the robust optimal control problem, could produce even greater synergistic improvements to the trajectory design. Additionally, the constant feedback gains were optimized for the initial guess trajectory, so further improvement is possible by reoptimizing the gains for the converged trajectory and control.

\subsection*{Comparison of UT and Monte Carlo Statistics}
\begin{table}[h!]
	\centering
	\includegraphics[width=0.75\textwidth]{ddp/table_mc}
	\caption{Summary of the statistics from the Unscented Transform and Monte Carlo}
	\label{table_mc}
\end{table}
\begin{figure}[h!]
	\centering
	\includegraphics[width=0.75\textwidth]{ddp/python/Altitude}
	\caption{Altitude versus velocity for 500 sample trajectories with Monte Carlo- and UT-estimated 3$\sigma$ bounds. The 3$ \sigma $-low altitude is 2.5 km.}
	\label{fig_mc_alt}
\end{figure}
\begin{figure}[h!]
	\centering
	\includegraphics[width=0.75\textwidth]{ddp/python/Range}
	\caption{Downrange versus velocity for 500 sample trajectories with Monte Carlo- and UT-estimated 3$\sigma$ bounds. The terminal 3$ \sigma $ range error is 1.8 km.}
	\label{fig_mc_range}
\end{figure}
\begin{figure}[h!]
	\centering
	\includegraphics[width=0.75\textwidth]{ddp/python/Acceleration}
	\caption{Acceleration load vs velocity for 500 sample trajectories with Monte Carlo- and UT-estimated 3$\sigma$ bounds.}
	\label{fig_mc_accel}
\end{figure}
\begin{figure}[h!]
	\centering
	\includegraphics[width=0.75\textwidth]{ddp/python/Control}
	\caption{The reference control and 500 sample trajectories.}
	\label{fig_mc_control}
\end{figure}

The previous subsections looked at broader trends when using the proposed method to design reference trajectories. This subsection will focus on a single solution, and compare the UT and Monte Carlo statistics in greater detail. Additionally, the feedback gains are again constants, but for this case they are reoptimized to minimize UT-estimated downrange errors after finding the optimal reference control with the previous gains. The reoptimized gains are $[k_D, k_{\gamma}, k_s] = [0.058, -0.021, -0.005]$. Since each component of the objective is important and a compromise must be met, we select the weights $w_h=w_s=1$. A Monte Carlo simulation is conducted with 8000 samples in order generate precise statistics for comparison with the unscented transform. 

%Reoptimization of the gains resulted in reduction in the 3$\sigma$ range error from 2.5 km to 1.8 km.

% Statistics comparison 
Table~\ref{table_mc} compares statistics estimated with the unscented transform and Monte Carlo simulation. The reference trajectory and constant gains are quite effective, with the Monte Carlo 3$ \sigma $ range error under 2 km, despite small estimation errors. This is achieved in part due to the reoptimization of the gains.
Relative to the Monte Carlo results, the UT overestimates the mean altitude by 100 m, the low altitude by 40 m, and underestimates the 3$ \sigma $ range error by 700 m, or nearly 40\%. Because the mean trajectory is used as the reference, the UT-estimated mean error is always zero.

Figures~\ref{fig_mc_alt}-\ref{fig_mc_accel} show 500 sample trajectories as well as the Monte Carlo estimated 3$\sigma$ bounds, and the UT-estimated 3$\sigma$ bounds for comparison. Each of these figures demonstrates that the UT-estimated statistics closely approximate the Monte Carlo statistics. Like the terminal statistics in Table~\ref{table_mc}, the UT estimation errors do not have a consistent sign at every point along a trajectory; on some intervals the UT underestimates the 3$\sigma$ bounds, while on other intervals it overestimates them. More importantly, the magnitude of these errors remains small, confirming that the UT is an effective method of quantifying uncertainty in the optimization process. 
% Solution features 
Figure~\ref{fig_mc_alt} shows that lofting at low velocities is a key feature of the trajectories used to raise the terminal altitude. Contrary to prior studies that found range control is not effective at low velocities \cite{MSL_EDL2}, Fig.~\ref{fig_mc_range} shows the convergence of the 3$\sigma$ range errors from 25 km at $ v=1100 $ m/s to less than 2 km at the $v=460$ m/s.  Figure~\ref{fig_mc_accel} shows the acceleration loads which never exceed 12g. If, as with MSL, a 15g limit is imposed, then this result indicates that a steeper mean entry flight path angle can be accommodated.

Figure~\ref{fig_mc_control} shows the reference control as well as the sample controls for the same 500 trajectory subset. The reference control resembles min-max profiles at high velocities before transitioning gradually to a full lift up orientation. While many of the samples have saturated controls over several intervals of the trajectory, there also exist intervals with significant margin, particularly between 1000 m/s and 3000 m/s. This is important because a full lift up orientation leaves no control authority for lateral guidance to manage the vehicle crossrange distance to the target.

\section*{Conclusion}
%Although a conclusion may review the main points of the paper, it must not replicate the abstract. A conclusion
%might elaborate on the importance of the work or suggest applications and extensions. Do not cite references in the
%conclusion. Note that the conclusion section is the last section of the paper to be numbered. The appendix (if present), funding information, other acknowledgments, and references are listed without numbers

In order to land at high elevations on Mars, entry guidance must deliver the vehicle to a sufficiently high altitude while also reducing the size of the landing footprint. 
As an alternative to the MSL range controller, we have presented a guidance approach that addresses both of these objectives. The entry guidance problem is posed as a robust optimal control problem with uncertain initial state and parameters. By augmenting the mean terminal altitude maximization objective with minimizing the standard deviations of altitude and downrange distance, reference entry trajectories robust to the modeled uncertainties are generated. The statistics required to evaluate the objective function are estimated with sample trajectories computed via the unscented transform. Differential dynamic programming is demonstrated to be an effective way to solve the resulting large-scale optimal control problem. Monte Carlo simulations verified the performance of the reference trajectories and controls. 
By explicitly incorporating uncertainty during the design of the reference trajectory and control, both the high elevation and range objectives are achieved robustly.

%\appendix
%\section*{Altitude Rate Feedback Conversion}
%There exists a velocity-varying gain $k_{\dot{h}}(v)$ such that $k_{\dot{h}}(v)\delta\dot{h}(v) = k_{\gamma}\delta\gamma(v)\;\forall\,\gamma,\, \dot{h}$.
%Proof: 
%\begin{align}
%k_{\dot{h}}(v)\delta\dot{h}(v) &= k_{\gamma}\delta\gamma(v) \\
%k_{\dot{h}}(v) &= k_{\gamma}\frac{\delta\gamma(v)}{\delta\dot{h}(v)} \\
%\dot{h} &= v\sin\gamma \\
%\delta \dot{h} &= v\delta\sin\gamma \\
%k_{\dot{h}}(v) &= \frac{k_{\gamma}}{v}\frac{\delta\gamma}{\delta\sin\gamma} \\
%\delta\sin\gamma &= 2\cos\frac{\gamma+\gamma_r}{2}\sin\frac{\gamma-\gamma_r}{2} \\
%\delta\sin\gamma &\approx \cos\gamma_r(\gamma-\gamma_r) = \cos\gamma_r\delta\gamma\\
%k_{\dot{h}}(v) &\approx \frac{k_{\gamma}}{v\cos\gamma_r(v)}
%\end{align}
%The final approximation introduces at most 3\% error for a flight path deviation of 10$^\circ$ (a fairly large deviation from the reference). The worst case is a negative 10$^\circ$ deviation from an already steep reference FPA (~-16$^\circ$)
\bibliography{bib}

\end{document}